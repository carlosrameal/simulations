---
title: "Estimator Trials"
output:
  pdf_document: default
  html_document: default
date: "2025-03-25"
bibliography: "C:/Users/carlorod/OneDrive - UGent/Documents/My_Library.bib"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(sandwich)   # For Huber-White (robust) standard errors
library(boot) 
library(dplyr)
library(MASS)
library(doParallel)
library(ggplot2)
library(reshape2)
library(lmtest)
```


```{r}
jackknife_po_fn <- function(data, indices) {
  # Leave-one-out data
  Y_loo <- data$Y[indices]
  X_loo <- as.matrix(data[indices, -1, drop = FALSE])
  
  # First-step regressions (partialling out)
  mY_loo <- lm(Y_loo ~ X_loo[,-1])
  mX1_loo <- lm(X_loo[,1] ~ X_loo[,-1])
  
  # Get residualized variables
  Y_tilde_loo <- mY_loo$residuals
  X1_tilde_loo <- mX1_loo$residuals
  
  # Second-step regression
  m2_loo <- lm(Y_tilde_loo ~ X1_tilde_loo - 1)
  
  return(coef(m2_loo)[1])  # Return beta estimate
}
```

## Linear model with independent predictors and errors (n=100)

### Case for p/n very high (p/n=0.9)
First we generate a population of n individuals with p regressors: 

```{r model generation}
set.seed(1)
n <- 100
k <- 1.1 # number of observations per coefficient
p <- floor(n/k) #number of variables
beta <- rep(0.5, p)
X <- replicate(p, rnorm(n)) # simplified setting: all predictors are orthogonal 
Y <- X%*%beta + rnorm(n)
```

We fit first an OLS model with intercept which yields quite bad results:
```{r OLS model}
m1 <- lm(Y ~ X)
coef(m1)[1:10]

```
We compute the estimate for X1 when partialling out (FWL theorem).
```{r FWL for X1 coefficient }
mY <- lm(Y ~ X[,-1])
mX1 <- lm(X[,1] ~ X[,-1])
Y.tilde <- mY$residuals
X1.tilde <- mX1$residuals
m2 <- lm(Y.tilde ~ X1.tilde -1 ) # no intercept 
```
OLS:
```{r  }
coef(summary(m1))[2,]
```

Partialllng out:
```{r }
coef(summary(m2))
```
Thanks to the FWL theorem we knew that the values of the estimates had to be the same. And due to the ratio p/n being small, the estimate is far from the real value of 0.5. We also see that the estimated S.E. is different in each model, with OLS's being higher and therefore more conservative although the real estimate would still be outside of the CI.The S.E. in the PO model is not expected to work well when p/n is very high as in this case.

**NOTE:** I think that there is a lot to consider here. In general, even if p/n is small, you should not use the standard SE estimator because of the homokedasticity assumption. The problem is that even if your original model is homokedastic, nothing ensures you that the second step PO model will also be. It would always be better to use the sanwich estimator (Huber-White robust s.e.) or the jackknife estimator when partialling out. I have the intuition that in this case in which every regressor is independent to each other we do not introduce homokedasticiy to the second model and therefore both the usual s.e. and the Sandwich estimators behave very similarly. I should recheck this when I introduce correlated regressors and possibly theoretically. Apart from this, because p/n is very big, even robust s.e. will not behave correctly as mentioned in the manual (refering to the sandwich estimator: "this standard error estimator formally works when p/n â‰ˆ 0,  but fails in settings where p/n is not small") 

Now let's simulate the results for X1 a thousand times to get an idea of the performance of the different s.e. estimators:

```{r}
# Detect available cores
num_cores <- detectCores() - 1  # Use available cores minus one to avoid system overload
cl <- makeCluster(num_cores)
registerDoParallel(cl)

# Number of simulations
N.sim <- 1000

# Run parallel computation
results <- foreach(i = 1:N.sim, .combine = rbind, .packages = c("sandwich", "boot")) %dopar% {
  
  X <- replicate(p, rnorm(n)) 
  Y <- X %*% beta + rnorm(n)

  # OLS Model
  m1 <- lm(Y ~ X)
  mY <- lm(Y ~ X[,-1])
  mX1 <- lm(X[,1] ~ X[,-1])
  Y.tilde <- mY$residuals
  X1.tilde <- mX1$residuals
  m2 <- lm(Y.tilde ~ X1.tilde -1)

  # Compute coefficients and standard errors
  beta_LR <- coef(summary(m1))[2,1]
  SE_LR <- coef(summary(m1))[2,2]
  SE_LRHW <- sqrt(diag(vcovHC(m1, type = "HC0"))[2])  # Huber-White SE
  SE_LRJN <- sqrt(diag(vcovHC(m1, type = "HC3"))[2])  # Jackknife SE
  
  beta_PO <- coef(summary(m2))[1]
  SE_PO <- coef(summary(m2))[2]
  SE_POHW <- sqrt(vcovHC(m2, type = "HC0"))  # Huber-White SE
  SE_POJN <- sqrt(vcovHC(m2, type = "HC3"))  # Jackknife SE

  # Jackknife Resampling
  data_list <- data.frame(Y = Y, X)
  jack_res <- boot(data = data_list, 
                   statistic = jackknife_po_fn, 
                   R = n,  
                   sim = "ordinary")
  SE_POJN2 <- sqrt((n - 1) / n * var(jack_res$t))
  
  # Return results as a row
  c(beta_LR, SE_LR, SE_LRHW, SE_LRJN, beta_PO, SE_PO, SE_POHW, SE_POJN, SE_POJN2)
}

# Stop the parallel cluster
stopCluster(cl)

# Convert results to a data frame
results_df <- as.data.frame(results)
colnames(results_df) <- c("beta.LR", "SE.LR", "SE.LRHW", "SE.LRJN", "beta.PO", "SE.PO", "SE.POHW", "SE.POJN", "SE.POJN2")

```


```{r,echo=FALSE}
results_table <- rbind(
  Mean = colMeans(results_df, na.rm = TRUE),  
  SD = apply(results_df, 2, sd, na.rm = TRUE)  
)
results_table
```

Indeed, we see that the usual OLS S.E. has a very good performance when compared to the Monte Carlo estimator. The three S.E. computed through the PO model are quite far from the actual one. The fact that they seem to be consistently smaller is very problematic if we wanted to compute confidence intervals.

**NOTE:** When performing the jackknife estimator when partialling out we should first remove the correspondent observation for the Leave-one-out and then compute both steps of the method. Applying HC3 to the second regression does not really function as a jackknife for the PO model.

```{r,echo=FALSE}
# Convert results_df to long format for ggplot
results_long <- melt(results_df[, c("SE.LR", "SE.LRHW", "SE.LRJN", "SE.PO", "SE.POHW", "SE.POJN")])

# Identify JN SE variables
JN_vars <- c("SE.LRJN", "SE.POJN")

# Compute the 95th percentile **only** for JN SE variables
quantiles_JN <- results_long %>%
  filter(variable %in% JN_vars) %>%
  group_by(variable) %>%
  summarise(upper = quantile(value, 0.95, na.rm = TRUE))

# Merge quantiles only for JN variables
results_filtered <- results_long %>%
  left_join(quantiles_JN, by = "variable") %>%
  mutate(value = ifelse(variable %in% JN_vars & value > upper, NA, value)) %>%
  filter(!is.na(value))  # Remove NA values from JN variables

# Compute the standard deviation of beta.LR
beta_LR_sd <- sd(results_df$beta.LR, na.rm = TRUE)

# Plot histograms with individual y-axes
ggplot(results_filtered, aes(x = value)) +
  geom_histogram(binwidth = 0.025, fill = "blue", alpha = 0.6, color = "black") +
  facet_wrap(~variable, scales = "free") +  # Free scales for both x and y axes
  geom_vline(xintercept = beta_LR_sd, color = "red", linetype = "dashed", size = 1) + # Red vertical line
  theme_minimal() +
  labs(title = "Histograms of Standard Errors (Top 5% Removed for Jackknife)", 
       x = "Value", y = "Frequency")
```

### Case for p/n high (p/n=0.5)

```{r ,echo=FALSE}
set.seed(1)
n <- 100
k <- 2 # number of observations per coefficient
p <- floor(n/k) #number of variables
beta <- rep(0.5, p)
X <- replicate(p, rnorm(n)) # simplified setting: all predictors are orthogonal 
Y <- X%*%beta + rnorm(n)
```

We fit first an OLS model with intercept which yields somewhat better results:
```{r  ,echo=FALSE}
m1 <- lm(Y ~ X)
coef(m1)[1:10]

```
We compute the estimate for X1 when partialling out (FWL theorem).
```{r,echo=FALSE}
mY <- lm(Y ~ X[,-1])
mX1 <- lm(X[,1] ~ X[,-1])
Y.tilde <- mY$residuals
X1.tilde <- mX1$residuals
m2 <- lm(Y.tilde ~ X1.tilde -1 ) # no intercept 

```
OLS:
```{r  ,echo=FALSE}
coef(summary(m1))[2,]

```

Partialllng out:
```{r ,echo=FALSE}
coef(summary(m2))
```
Now, due to the ratio p/n being bigger, the estimate is much closer to the real value of 0.5. We continue to see that the estimated S.D. is different in each model, with OLS's again being higher and therefore more conservative. In this case both C.I. encompass the real value. 

Let's run the simulations:

```{r, echo=FALSE}
# Detect available cores
num_cores <- detectCores() - 1  # Use available cores minus one to avoid system overload
cl <- makeCluster(num_cores)
registerDoParallel(cl)

# Number of simulations
N.sim <- 1000

# Run parallel computation
results <- foreach(i = 1:N.sim, .combine = rbind, .packages = c("sandwich", "boot")) %dopar% {
  
  X <- replicate(p, rnorm(n)) 
  Y <- X %*% beta + rnorm(n)

  # OLS Model
  m1 <- lm(Y ~ X)
  mY <- lm(Y ~ X[,-1])
  mX1 <- lm(X[,1] ~ X[,-1])
  Y.tilde <- mY$residuals
  X1.tilde <- mX1$residuals
  m2 <- lm(Y.tilde ~ X1.tilde -1)

  # Compute coefficients and standard errors
  beta_LR <- coef(summary(m1))[2,1]
  SE_LR <- coef(summary(m1))[2,2]
  SE_LRHW <- sqrt(diag(vcovHC(m1, type = "HC0"))[2])  # Huber-White SE
  SE_LRJN <- sqrt(diag(vcovHC(m1, type = "HC3"))[2])  # Jackknife SE
  
  beta_PO <- coef(summary(m2))[1]
  SE_PO <- coef(summary(m2))[2]
  SE_POHW <- sqrt(vcovHC(m2, type = "HC0"))  # Huber-White SE
  SE_POJN <- sqrt(vcovHC(m2, type = "HC3"))  # Jackknife SE

  # Jackknife Resampling
  data_list <- data.frame(Y = Y, X)
  jack_res <- boot(data = data_list, 
                   statistic = jackknife_po_fn, 
                   R = n,  
                   sim = "ordinary")
  SE_POJN2 <- sqrt((n - 1) / n * var(jack_res$t))
  
  # Return results as a row
  c(beta_LR, SE_LR, SE_LRHW, SE_LRJN, beta_PO, SE_PO, SE_POHW, SE_POJN, SE_POJN2)
}

# Stop the parallel cluster
stopCluster(cl)

# Convert results to a data frame
results_df <- as.data.frame(results)
colnames(results_df) <- c("beta.LR", "SE.LR", "SE.LRHW", "SE.LRJN", "beta.PO", "SE.PO", "SE.POHW", "SE.POJN", "SE.POJN2")
```

```{r,echo=FALSE}
results_table <- rbind(
  Mean = colMeans(results_df, na.rm = TRUE),  
  SD = apply(results_df, 2, sd, na.rm = TRUE)  
)
results_table
```

The LR s.e. continues to greatly outperform the PO s.e. although this time they are significantly closer. The three PO s.e. continue to behave similarly, which to me points even more to homokedasticity due to how the regressors were created.

```{r,echo=FALSE}
# Convert results_df to long format for ggplot
results_long <- melt(results_df[, c("SE.LR", "SE.LRHW", "SE.LRJN", "SE.PO", "SE.POHW", "SE.POJN")])

# Identify JN SE variables
JN_vars <- c("SE.LRJN", "SE.POJN")

# Compute the 95th percentile **only** for JN SE variables
quantiles_JN <- results_long %>%
  filter(variable %in% JN_vars) %>%
  group_by(variable) %>%
  summarise(upper = quantile(value, 0.95, na.rm = TRUE))

# Merge quantiles only for JN variables
results_filtered <- results_long %>%
  left_join(quantiles_JN, by = "variable") %>%
  mutate(value = ifelse(variable %in% JN_vars & value > upper, NA, value)) %>%
  filter(!is.na(value))  # Remove NA values from JN variables

# Compute the standard deviation of beta.LR
beta_LR_sd <- sd(results_df$beta.LR, na.rm = TRUE)

# Plot histograms with individual y-axes
ggplot(results_filtered, aes(x = value)) +
  geom_histogram(binwidth = 0.1, fill = "blue", alpha = 0.6, color = "black") +
  facet_wrap(~variable, scales = "free") +  # Free scales for both x and y axes
  geom_vline(xintercept = beta_LR_sd, color = "red", linetype = "dashed", size = 1) + # Red vertical line
  theme_minimal() +
  labs(title = "Histograms of Standard Errors (Top 5% Removed for Jackknife)", 
       x = "Value", y = "Frequency")
```

### Case for p/n small (p/n=0.05)

```{r ,echo=FALSE}
set.seed(1)
n <- 100
k <- 20 # number of observations per coefficient
p <- floor(n/k) #number of variables
beta <- rep(0.5, p)
X <- replicate(p, rnorm(n)) # simplified setting: all predictors are orthogonal 
Y <- X%*%beta + rnorm(n)
```

We fit first an OLS model with intercept which yields much better results:
```{r  ,echo=FALSE}
m1 <- lm(Y ~ X)
coef(m1)[1:6]

```

We compute the estimate for X1 when partialling out (FWL theorem).
```{r,echo=FALSE}
mY <- lm(Y ~ X[,-1])
mX1 <- lm(X[,1] ~ X[,-1])
Y.tilde <- mY$residuals
X1.tilde <- mX1$residuals
m2 <- lm(Y.tilde ~ X1.tilde -1 ) # no intercept 

```
OLS:
```{r  ,echo=FALSE}
coef(summary(m1))[2,]

```

Partialllng out:
```{r ,echo=FALSE}
coef(summary(m2))
```
This time in particular X1's estimate is the worse out of the 5. It is worse than for the previous case where p/n=0.5, although in general the model worked better now. The SD of normal OLS and Partialling out now are very similar and the real value is inside the C.I.

```{r, echo=FALSE}
# Detect available cores
num_cores <- detectCores() - 1  # Use available cores minus one to avoid system overload
cl <- makeCluster(num_cores)
registerDoParallel(cl)

# Number of simulations
N.sim <- 1000

# Run parallel computation
results <- foreach(i = 1:N.sim, .combine = rbind, .packages = c("sandwich", "boot")) %dopar% {
  
  X <- replicate(p, rnorm(n)) 
  Y <- X %*% beta + rnorm(n)

  # OLS Model
  m1 <- lm(Y ~ X)
  mY <- lm(Y ~ X[,-1])
  mX1 <- lm(X[,1] ~ X[,-1])
  Y.tilde <- mY$residuals
  X1.tilde <- mX1$residuals
  m2 <- lm(Y.tilde ~ X1.tilde -1)

  # Compute coefficients and standard errors
  beta_LR <- coef(summary(m1))[2,1]
  SE_LR <- coef(summary(m1))[2,2]
  SE_LRHW <- sqrt(diag(vcovHC(m1, type = "HC0"))[2])  # Huber-White SE
  SE_LRJN <- sqrt(diag(vcovHC(m1, type = "HC3"))[2])  # Jackknife SE
  
  beta_PO <- coef(summary(m2))[1]
  SE_PO <- coef(summary(m2))[2]
  SE_POHW <- sqrt(vcovHC(m2, type = "HC0"))  # Huber-White SE
  SE_POJN <- sqrt(vcovHC(m2, type = "HC3"))  # Jackknife SE

  # Jackknife Resampling
  data_list <- data.frame(Y = Y, X)
  jack_res <- boot(data = data_list, 
                   statistic = jackknife_po_fn, 
                   R = n,  
                   sim = "ordinary")
  SE_POJN2 <- sqrt((n - 1) / n * var(jack_res$t))
  
  # Return results as a row
  c(beta_LR, SE_LR, SE_LRHW, SE_LRJN, beta_PO, SE_PO, SE_POHW, SE_POJN, SE_POJN2)
}

# Stop the parallel cluster
stopCluster(cl)

# Convert results to a data frame
results_df <- as.data.frame(results)
colnames(results_df) <- c("beta.LR", "SE.LR", "SE.LRHW", "SE.LRJN", "beta.PO", "SE.PO", "SE.POHW", "SE.POJN", "SE.POJN2")

```

```{r,echo=FALSE}
results_table <- rbind(
  Mean = colMeans(results_df, na.rm = TRUE),  
  SD = apply(results_df, 2, sd, na.rm = TRUE)  
)
results_table
```

Now that p/n can be considered small (although usually we would want p/n<0.1), the s.e. derived from the PO model are very similar to the usual s.e. and also to the monte carlo s.d. This little exercise clearly shows the problem of using inference when using PO if p/n is big. 

```{r,echo=FALSE}
# Convert results_df to long format for ggplot
results_long <- melt(results_df[, c("SE.LR", "SE.LRHW", "SE.LRJN", "SE.PO", "SE.POHW", "SE.POJN")])

# Identify JN SE variables
JN_vars <- c("SE.LRJN", "SE.POJN")

# Compute the 95th percentile **only** for JN SE variables
quantiles_JN <- results_long %>%
  filter(variable %in% JN_vars) %>%
  group_by(variable) %>%
  summarise(upper = quantile(value, 0.95, na.rm = TRUE))

# Merge quantiles only for JN variables
results_filtered <- results_long %>%
  left_join(quantiles_JN, by = "variable") %>%
  mutate(value = ifelse(variable %in% JN_vars & value > upper, NA, value)) %>%
  filter(!is.na(value))  # Remove NA values from JN variables

# Compute the standard deviation of beta.LR
beta_LR_sd <- sd(results_df$beta.LR, na.rm = TRUE)

# Plot histograms with individual y-axes
ggplot(results_filtered, aes(x = value)) +
  geom_histogram(binwidth = 0.1, fill = "blue", alpha = 0.6, color = "black") +
  facet_wrap(~variable, scales = "free") +  # Free scales for both x and y axes
  geom_vline(xintercept = beta_LR_sd, color = "red", linetype = "dashed", size = 1) + # Red vertical line
  theme_minimal() +
  labs(title = "Histograms of Standard Errors (Top 5% Removed for Jackknife)", 
       x = "Value", y = "Frequency")
```

